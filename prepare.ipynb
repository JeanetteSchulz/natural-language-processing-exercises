{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e686dba8",
   "metadata": {},
   "source": [
    "# NLP: Prepare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "514d82d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "import acquire\n",
    "from time import strftime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07e6d79",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "The end result of this exercise should be a file named `prepare.py` that defines the requested functions.\n",
    "\n",
    "In this exercise we will be defining some functions to prepare textual data. These functions should apply equally well to both the codeup blog articles and the news articles that were previously acquired.\n",
    "\n",
    "1. Define a function named basic_clean. It should take in a string and apply some basic text cleaning to it:\n",
    "\n",
    "- Lowercase everything\n",
    "- Normalize unicode characters\n",
    "- Replace anything that is not a letter, number, whitespace or a single quote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "128c6c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(string):\n",
    "    '''\n",
    "    This function takes in a string and\n",
    "    returns the string normalized.\n",
    "    '''\n",
    "    string = unicodedata.normalize('NFKD', string)\\\n",
    "             .encode('ascii', 'ignore')\\\n",
    "             .decode('utf-8', 'ignore')\n",
    "    string = re.sub(r'[^\\w\\s]', '', string).lower()\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28c90bb",
   "metadata": {},
   "source": [
    "2. Define a function named `tokenize`. It should take in a string and tokenize all the words in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52b97b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    '''\n",
    "    This function takes in a string and\n",
    "    returns a tokenized string.\n",
    "    '''\n",
    "    # Create tokenizer.\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    \n",
    "    # Use tokenizer\n",
    "    string = tokenizer.tokenize(string, return_str = True)\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7c0b5d",
   "metadata": {},
   "source": [
    "3. Define a function named `stem`. It should accept some text and return the text after applying stemming to all the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98f81121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(string):\n",
    "    '''\n",
    "    This function takes in a string and\n",
    "    returns a string with words stemmed.\n",
    "    '''\n",
    "    # Create porter stemmer.\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    \n",
    "    # Use the stemmer to stem each word in the list of words we created by using split.\n",
    "    stems = [ps.stem(word) for word in string.split()]\n",
    "    \n",
    "    # Join our lists of words into a string again and assign to a variable.\n",
    "    string = ' '.join(stems)\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadaa4b3",
   "metadata": {},
   "source": [
    "4. Define a function named `lemmatize`. It should accept some text and return the text after applying lemmatization to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25b212ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(string):\n",
    "    '''\n",
    "    This function takes in string for and\n",
    "    returns a string with words lemmatized.\n",
    "    '''\n",
    "    # Create the lemmatizer.\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    # Use the lemmatizer on each word in the list of words we created by using split.\n",
    "    lemmas = [wnl.lemmatize(word) for word in string.split()]\n",
    "    \n",
    "    # Join our list of words into a string again and assign to a variable.\n",
    "    string = ' '.join(lemmas)\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff5f158",
   "metadata": {},
   "source": [
    "5. Define a function named `remove_stopwords`. It should accept some text and return the text after removing all the stopwords.     This function should define two optional parameters, extra_words and exclude_words. These parameters should define any additional stop words to include, and any words that we don't want to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0191db45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(string, extra_words = [], exclude_words = []):\n",
    "    '''\n",
    "    This function takes in a string, optional extra_words and exclude_words parameters\n",
    "    with default empty lists and returns a string.\n",
    "    '''\n",
    "    # Create stopword_list.\n",
    "    stopword_list = stopwords.words('english')\n",
    "    \n",
    "    # Remove 'exclude_words' from stopword_list to keep these in my text.\n",
    "    stopword_list = set(stopword_list) - set(exclude_words)\n",
    "    \n",
    "    # Add in 'extra_words' to stopword_list.\n",
    "    stopword_list = stopword_list.union(set(extra_words))\n",
    "\n",
    "    # Split words in string.\n",
    "    words = string.split()\n",
    "    \n",
    "    # Create a list of words from my string with stopwords removed and assign to variable.\n",
    "    filtered_words = [word for word in words if word not in stopword_list]\n",
    "    \n",
    "    # Join words in the list back into strings and assign to a variable.\n",
    "    string_without_stopwords = ' '.join(filtered_words)\n",
    "    \n",
    "    return string_without_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301b8356",
   "metadata": {},
   "source": [
    "6. Use your data from the acquire to produce a dataframe of the news articles. Name the dataframe `news_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67229388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use get_all_new_article function from acquire.py file \n",
    "news_df = acquire.get_inshorts_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7ebc5f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ambani, Adani become richer than Zuckerberg af...</td>\n",
       "      <td>Pragya Swastik</td>\n",
       "      <td>Reliance Industries Chairman Mukesh Ambani and...</td>\n",
       "      <td>04 Feb 2022,Friday</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drop in Meta's market value more than the tota...</td>\n",
       "      <td>Arshiya Chopra</td>\n",
       "      <td>After Facebook parent Meta lost $251 billion i...</td>\n",
       "      <td>04 Feb 2022,Friday</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazon adds $135 bn in one of the biggest 1-da...</td>\n",
       "      <td>Hiral Goyal</td>\n",
       "      <td>Amazon added more than $135 billion in market ...</td>\n",
       "      <td>04 Feb 2022,Friday</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Meta drops below Berkshire Hathaway in market ...</td>\n",
       "      <td>Hiral Goyal</td>\n",
       "      <td>Meta Platforms is now worth about $50 billion ...</td>\n",
       "      <td>04 Feb 2022,Friday</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Facebook's user growth in India slowed due to ...</td>\n",
       "      <td>Sakshita Khosla</td>\n",
       "      <td>Facebook's user growth in India was hit due to...</td>\n",
       "      <td>04 Feb 2022,Friday</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title           author  \\\n",
       "0  Ambani, Adani become richer than Zuckerberg af...   Pragya Swastik   \n",
       "1  Drop in Meta's market value more than the tota...   Arshiya Chopra   \n",
       "2  Amazon adds $135 bn in one of the biggest 1-da...      Hiral Goyal   \n",
       "3  Meta drops below Berkshire Hathaway in market ...      Hiral Goyal   \n",
       "4  Facebook's user growth in India slowed due to ...  Sakshita Khosla   \n",
       "\n",
       "                                             content                date  \\\n",
       "0  Reliance Industries Chairman Mukesh Ambani and...  04 Feb 2022,Friday   \n",
       "1  After Facebook parent Meta lost $251 billion i...  04 Feb 2022,Friday   \n",
       "2  Amazon added more than $135 billion in market ...  04 Feb 2022,Friday   \n",
       "3  Meta Platforms is now worth about $50 billion ...  04 Feb 2022,Friday   \n",
       "4  Facebook's user growth in India was hit due to...  04 Feb 2022,Friday   \n",
       "\n",
       "   category  \n",
       "0  business  \n",
       "1  business  \n",
       "2  business  \n",
       "3  business  \n",
       "4  business  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58004e47",
   "metadata": {},
   "source": [
    "7. Make another dataframe for the Codeup blog posts. Name the dataframe `codeup_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5d71c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "codeup_df = acquire.get_codeup_blogs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a06361b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>published</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Codeup Dallas Open House</td>\n",
       "      <td>Nov 30, 2021</td>\n",
       "      <td>Come join us for the re-opening of our Dallas ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Codeup’s Placement Team Continues Setting Records</td>\n",
       "      <td>Nov 19, 2021</td>\n",
       "      <td>Our Placement Team is simply defined as a grou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IT Certifications 101: Why They Matter, and Wh...</td>\n",
       "      <td>Nov 18, 2021</td>\n",
       "      <td>AWS, Google, Azure, Red Hat, CompTIA…these are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A rise in cyber attacks means opportunities fo...</td>\n",
       "      <td>Nov 17, 2021</td>\n",
       "      <td>In the last few months, the US has experienced...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Use your GI Bill® benefits to Land a Job in Tech</td>\n",
       "      <td>Nov 4, 2021</td>\n",
       "      <td>As the end of military service gets closer, ma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title     published  \\\n",
       "0                           Codeup Dallas Open House  Nov 30, 2021   \n",
       "1  Codeup’s Placement Team Continues Setting Records  Nov 19, 2021   \n",
       "2  IT Certifications 101: Why They Matter, and Wh...  Nov 18, 2021   \n",
       "3  A rise in cyber attacks means opportunities fo...  Nov 17, 2021   \n",
       "4   Use your GI Bill® benefits to Land a Job in Tech   Nov 4, 2021   \n",
       "\n",
       "                                             content  \n",
       "0  Come join us for the re-opening of our Dallas ...  \n",
       "1  Our Placement Team is simply defined as a grou...  \n",
       "2  AWS, Google, Azure, Red Hat, CompTIA…these are...  \n",
       "3  In the last few months, the US has experienced...  \n",
       "4  As the end of military service gets closer, ma...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codeup_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c6b542",
   "metadata": {},
   "source": [
    "8. For each dataframe, produce the following columns:\n",
    "- `title` to hold the title\n",
    "- `original` to hold the original article/post content\n",
    "- `clean` to hold the normalized and tokenized original with the stopwords removed.\n",
    "- `stemmed` to hold the stemmed version of the cleaned data.\n",
    "- `lemmatized` to hold the lemmatized version of the cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "909b4d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.rename(columns={'content': 'original'}, inplace=True)\n",
    "codeup_df.rename(columns={'content': 'original'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7387cb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_article_data(df, column, extra_words=[], exclude_words=[]):\n",
    "    '''\n",
    "    This function take in a df and the string name for a text column with \n",
    "    option to pass lists for extra_words and exclude_words and\n",
    "    returns a df with the text article title, original text, stemmed text,\n",
    "    lemmatized text, cleaned, tokenized, & lemmatized text with stopwords removed.\n",
    "    '''\n",
    "    df['clean'] = df[column].apply(basic_clean)\\\n",
    "                            .apply(tokenize)\\\n",
    "                            .apply(remove_stopwords, \n",
    "                                   extra_words=extra_words, \n",
    "                                   exclude_words=exclude_words)\n",
    "    \n",
    "    df['stemmed'] = df[column].apply(basic_clean)\\\n",
    "                            .apply(tokenize)\\\n",
    "                            .apply(stem)\\\n",
    "                            .apply(remove_stopwords, \n",
    "                                   extra_words=extra_words, \n",
    "                                   exclude_words=exclude_words)\n",
    "    \n",
    "    df['lemmatized'] = df[column].apply(basic_clean)\\\n",
    "                            .apply(tokenize)\\\n",
    "                            .apply(lemmatize)\\\n",
    "                            .apply(remove_stopwords, \n",
    "                                   extra_words=extra_words, \n",
    "                                   exclude_words=exclude_words)\n",
    "    \n",
    "    return df[['title', column,'clean', 'stemmed', 'lemmatized']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d24b8ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>original</th>\n",
       "      <th>clean</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ambani, Adani become richer than Zuckerberg af...</td>\n",
       "      <td>Reliance Industries Chairman Mukesh Ambani and...</td>\n",
       "      <td>reliance industries chairman mukesh ambani ada...</td>\n",
       "      <td>relianc industri chairman mukesh ambani adani ...</td>\n",
       "      <td>reliance industry chairman mukesh ambani adani...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drop in Meta's market value more than the tota...</td>\n",
       "      <td>After Facebook parent Meta lost $251 billion i...</td>\n",
       "      <td>facebook parent meta lost 251 billion market v...</td>\n",
       "      <td>facebook parent meta lost 251 billion market v...</td>\n",
       "      <td>facebook parent meta lost 251 billion market v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazon adds $135 bn in one of the biggest 1-da...</td>\n",
       "      <td>Amazon added more than $135 billion in market ...</td>\n",
       "      <td>amazon added 135 billion market value one bigg...</td>\n",
       "      <td>amazon ad 135 billion market valu one biggest ...</td>\n",
       "      <td>amazon added 135 billion market value one bigg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Meta drops below Berkshire Hathaway in market ...</td>\n",
       "      <td>Meta Platforms is now worth about $50 billion ...</td>\n",
       "      <td>meta platforms worth 50 billion less berkshire...</td>\n",
       "      <td>meta platform worth 50 billion less berkshir h...</td>\n",
       "      <td>meta platform worth 50 billion le berkshire ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Facebook's user growth in India slowed due to ...</td>\n",
       "      <td>Facebook's user growth in India was hit due to...</td>\n",
       "      <td>facebooks user growth india hit due hike prepa...</td>\n",
       "      <td>facebook user growth india wa hit due hike pre...</td>\n",
       "      <td>facebooks user growth india wa hit due hike pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Ambani, Adani become richer than Zuckerberg af...   \n",
       "1  Drop in Meta's market value more than the tota...   \n",
       "2  Amazon adds $135 bn in one of the biggest 1-da...   \n",
       "3  Meta drops below Berkshire Hathaway in market ...   \n",
       "4  Facebook's user growth in India slowed due to ...   \n",
       "\n",
       "                                            original  \\\n",
       "0  Reliance Industries Chairman Mukesh Ambani and...   \n",
       "1  After Facebook parent Meta lost $251 billion i...   \n",
       "2  Amazon added more than $135 billion in market ...   \n",
       "3  Meta Platforms is now worth about $50 billion ...   \n",
       "4  Facebook's user growth in India was hit due to...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  reliance industries chairman mukesh ambani ada...   \n",
       "1  facebook parent meta lost 251 billion market v...   \n",
       "2  amazon added 135 billion market value one bigg...   \n",
       "3  meta platforms worth 50 billion less berkshire...   \n",
       "4  facebooks user growth india hit due hike prepa...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  relianc industri chairman mukesh ambani adani ...   \n",
       "1  facebook parent meta lost 251 billion market v...   \n",
       "2  amazon ad 135 billion market valu one biggest ...   \n",
       "3  meta platform worth 50 billion less berkshir h...   \n",
       "4  facebook user growth india wa hit due hike pre...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  reliance industry chairman mukesh ambani adani...  \n",
       "1  facebook parent meta lost 251 billion market v...  \n",
       "2  amazon added 135 billion market value one bigg...  \n",
       "3  meta platform worth 50 billion le berkshire ha...  \n",
       "4  facebooks user growth india wa hit due hike pr...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the function defined above for news_df's content column.\n",
    "\n",
    "prep_article_data(news_df, 'original', extra_words = ['ha'], exclude_words = ['no']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc25f564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>original</th>\n",
       "      <th>clean</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Codeup Dallas Open House</td>\n",
       "      <td>Come join us for the re-opening of our Dallas ...</td>\n",
       "      <td>come join us reopening dallas campus drinks sn...</td>\n",
       "      <td>come join us reopen dalla campu drink snack co...</td>\n",
       "      <td>come join u reopening dallas campus drink snac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Codeup’s Placement Team Continues Setting Records</td>\n",
       "      <td>Our Placement Team is simply defined as a grou...</td>\n",
       "      <td>placement team simply defined group manages re...</td>\n",
       "      <td>placement team simpli defin group manag relati...</td>\n",
       "      <td>placement team simply defined group manages re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IT Certifications 101: Why They Matter, and Wh...</td>\n",
       "      <td>AWS, Google, Azure, Red Hat, CompTIA…these are...</td>\n",
       "      <td>aws google azure red hat comptiathese big name...</td>\n",
       "      <td>aw googl azur red hat comptiathes big name onl...</td>\n",
       "      <td>aws google azure red hat comptiathese big name...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A rise in cyber attacks means opportunities fo...</td>\n",
       "      <td>In the last few months, the US has experienced...</td>\n",
       "      <td>last months us experienced dozens major cybera...</td>\n",
       "      <td>last month us experienc dozen major cyberattac...</td>\n",
       "      <td>last month u experienced dozen major cyberatta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Use your GI Bill® benefits to Land a Job in Tech</td>\n",
       "      <td>As the end of military service gets closer, ma...</td>\n",
       "      <td>end military service gets closer many transiti...</td>\n",
       "      <td>end militari servic get closer mani transit se...</td>\n",
       "      <td>end military service get closer many transitio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                           Codeup Dallas Open House   \n",
       "1  Codeup’s Placement Team Continues Setting Records   \n",
       "2  IT Certifications 101: Why They Matter, and Wh...   \n",
       "3  A rise in cyber attacks means opportunities fo...   \n",
       "4   Use your GI Bill® benefits to Land a Job in Tech   \n",
       "\n",
       "                                            original  \\\n",
       "0  Come join us for the re-opening of our Dallas ...   \n",
       "1  Our Placement Team is simply defined as a grou...   \n",
       "2  AWS, Google, Azure, Red Hat, CompTIA…these are...   \n",
       "3  In the last few months, the US has experienced...   \n",
       "4  As the end of military service gets closer, ma...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  come join us reopening dallas campus drinks sn...   \n",
       "1  placement team simply defined group manages re...   \n",
       "2  aws google azure red hat comptiathese big name...   \n",
       "3  last months us experienced dozens major cybera...   \n",
       "4  end military service gets closer many transiti...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  come join us reopen dalla campu drink snack co...   \n",
       "1  placement team simpli defin group manag relati...   \n",
       "2  aw googl azur red hat comptiathes big name onl...   \n",
       "3  last month us experienc dozen major cyberattac...   \n",
       "4  end militari servic get closer mani transit se...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  come join u reopening dallas campus drink snac...  \n",
       "1  placement team simply defined group manages re...  \n",
       "2  aws google azure red hat comptiathese big name...  \n",
       "3  last month u experienced dozen major cyberatta...  \n",
       "4  end military service get closer many transitio...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the function defined above for codeup_df's content column.\n",
    "\n",
    "prep_article_data(codeup_df, 'original', extra_words = ['ha'], exclude_words = ['no']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81315ea3",
   "metadata": {},
   "source": [
    "Ask yourself:\n",
    "\n",
    "If your corpus is 493KB, would you prefer to use stemmed or lemmatized text?\n",
    "If your corpus is 25MB, would you prefer to use stemmed or lemmatized text?\n",
    "If your corpus is 200TB of text and you're charged by the megabyte for your hosted computational resources, would you prefer to use stemmed or lemmatized text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31671b08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
